{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d9d6d70",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data size: 5033\n",
      "Price             Close       \n",
      "Ticker            ^GSPC   ^VIX\n",
      "Date                          \n",
      "2005-01-03  1202.079956  14.08\n",
      "2005-01-04  1188.050049  13.98\n",
      "2005-01-05  1183.739990  14.09\n",
      "2005-01-06  1187.890015  13.58\n",
      "2005-01-07  1186.189941  13.49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'Close_sp500'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\danch\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3811\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3812\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._engine.get_loc(casted_key)\n\u001b[32m   3813\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:167\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:196\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7096\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mKeyError\u001b[39m: 'Close_sp500'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 25\u001b[39m\n\u001b[32m     23\u001b[39m \u001b[38;5;28mprint\u001b[39m(data.head())\n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m# Calculate log returns\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m data[\u001b[33m'\u001b[39m\u001b[33mlog_ret_sp500\u001b[39m\u001b[33m'\u001b[39m] = np.log(data[\u001b[33m'\u001b[39m\u001b[33mClose_sp500\u001b[39m\u001b[33m'\u001b[39m] / data[\u001b[33m'\u001b[39m\u001b[33mClose_sp500\u001b[39m\u001b[33m'\u001b[39m].shift(\u001b[32m1\u001b[39m))\n\u001b[32m     26\u001b[39m data[\u001b[33m'\u001b[39m\u001b[33mlog_ret_vix\u001b[39m\u001b[33m'\u001b[39m] = np.log(data[\u001b[33m'\u001b[39m\u001b[33mClose_vix\u001b[39m\u001b[33m'\u001b[39m] / data[\u001b[33m'\u001b[39m\u001b[33mClose_vix\u001b[39m\u001b[33m'\u001b[39m].shift(\u001b[32m1\u001b[39m))\n\u001b[32m     27\u001b[39m data.dropna(inplace=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\danch\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:4106\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4104\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_single_key:\n\u001b[32m   4105\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.columns.nlevels > \u001b[32m1\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m4106\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._getitem_multilevel(key)\n\u001b[32m   4107\u001b[39m     indexer = \u001b[38;5;28mself\u001b[39m.columns.get_loc(key)\n\u001b[32m   4108\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\danch\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:4164\u001b[39m, in \u001b[36mDataFrame._getitem_multilevel\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4162\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34m_getitem_multilevel\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):\n\u001b[32m   4163\u001b[39m     \u001b[38;5;66;03m# self.columns is a MultiIndex\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m4164\u001b[39m     loc = \u001b[38;5;28mself\u001b[39m.columns.get_loc(key)\n\u001b[32m   4165\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(loc, (\u001b[38;5;28mslice\u001b[39m, np.ndarray)):\n\u001b[32m   4166\u001b[39m         new_columns = \u001b[38;5;28mself\u001b[39m.columns[loc]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\danch\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\multi.py:3059\u001b[39m, in \u001b[36mMultiIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3056\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m mask\n\u001b[32m   3058\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m3059\u001b[39m     loc = \u001b[38;5;28mself\u001b[39m._get_level_indexer(key, level=\u001b[32m0\u001b[39m)\n\u001b[32m   3060\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _maybe_to_slice(loc)\n\u001b[32m   3062\u001b[39m keylen = \u001b[38;5;28mlen\u001b[39m(key)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\danch\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\multi.py:3410\u001b[39m, in \u001b[36mMultiIndex._get_level_indexer\u001b[39m\u001b[34m(self, key, level, indexer)\u001b[39m\n\u001b[32m   3407\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mslice\u001b[39m(i, j, step)\n\u001b[32m   3409\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3410\u001b[39m     idx = \u001b[38;5;28mself\u001b[39m._get_loc_single_level_index(level_index, key)\n\u001b[32m   3412\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m level > \u001b[32m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._lexsort_depth == \u001b[32m0\u001b[39m:\n\u001b[32m   3413\u001b[39m         \u001b[38;5;66;03m# Desired level is not sorted\u001b[39;00m\n\u001b[32m   3414\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(idx, \u001b[38;5;28mslice\u001b[39m):\n\u001b[32m   3415\u001b[39m             \u001b[38;5;66;03m# test_get_loc_partial_timestamp_multiindex\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\danch\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\multi.py:2999\u001b[39m, in \u001b[36mMultiIndex._get_loc_single_level_index\u001b[39m\u001b[34m(self, level_index, key)\u001b[39m\n\u001b[32m   2997\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m -\u001b[32m1\u001b[39m\n\u001b[32m   2998\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2999\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m level_index.get_loc(key)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\danch\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3819\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3814\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   3815\u001b[39m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc.Iterable)\n\u001b[32m   3816\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[32m   3817\u001b[39m     ):\n\u001b[32m   3818\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[32m-> \u001b[39m\u001b[32m3819\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01merr\u001b[39;00m\n\u001b[32m   3820\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   3821\u001b[39m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[32m   3822\u001b[39m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[32m   3823\u001b[39m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[32m   3824\u001b[39m     \u001b[38;5;28mself\u001b[39m._check_indexing_error(key)\n",
      "\u001b[31mKeyError\u001b[39m: 'Close_sp500'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from arch import arch_model\n",
    "\n",
    "# Download and align data\n",
    "start_date = '2005-01-01'\n",
    "end_date = '2025-01-01'\n",
    "\n",
    "# Fetch data with identical dates\n",
    "vix = yf.download('^VIX', start=start_date, end=end_date, auto_adjust=True)\n",
    "sp500 = yf.download('^GSPC', start=start_date, end=end_date, auto_adjust=True)\n",
    "\n",
    "# Merge to ensure date alignment\n",
    "data = pd.merge(sp500[['Close']], vix[['Close']], left_index=True, right_index=True, \n",
    "                suffixes=('_sp500', '_vix'))\n",
    "print(f\"Original data size: {len(data)}\")\n",
    "print(data.head())\n",
    "# Calculate log returns\n",
    "data['log_ret_sp500'] = np.log(data['Close_sp500'] / data['Close_sp500'].shift(1))\n",
    "data['log_ret_vix'] = np.log(data['Close_vix'] / data['Close_vix'].shift(1))\n",
    "data.dropna(inplace=True)\n",
    "print(f\"Aligned data size after dropna: {len(data)}\")\n",
    "\n",
    "# Fit GARCH on S&P500 log returns\n",
    "garch_model = arch_model(\n",
    "    data['log_ret_sp500'], \n",
    "    mean='Constant',  # Explicitly model mean\n",
    "    vol='GARCH', \n",
    "    p=1, \n",
    "    q=1\n",
    ")\n",
    "garch_result = garch_model.fit(update_freq=0, disp='off')\n",
    "print(garch_result.summary())\n",
    "\n",
    "# Get standardized residuals\n",
    "data['garch_resid'] = garch_result.std_resid\n",
    "\n",
    "# Split data before scaling (avoid leakage)\n",
    "split_idx = int(0.6 * len(data))\n",
    "train_data = data.iloc[:split_idx]\n",
    "test_data = data.iloc[split_idx:]\n",
    "\n",
    "# Scale VIX log returns using training data only\n",
    "vix_scaler = StandardScaler()\n",
    "train_data['log_ret_vix_scaled'] = vix_scaler.fit_transform(train_data[['log_ret_vix']])\n",
    "test_data['log_ret_vix_scaled'] = vix_scaler.transform(test_data[['log_ret_vix']])\n",
    "\n",
    "# Create sequences [GARCH residual, Scaled VIX return] -> next VIX return\n",
    "SEQ_LEN = 30\n",
    "\n",
    "def create_sequences(df):\n",
    "    xs, ys, prices = [], [], []\n",
    "    for i in range(SEQ_LEN, len(df)):\n",
    "        xs.append(df[['garch_resid', 'log_ret_vix_scaled']].iloc[i-SEQ_LEN:i].values)\n",
    "        ys.append(df['log_ret_vix_scaled'].iloc[i])\n",
    "        prices.append(df['Close_vix'].iloc[i-1])  # Previous close for price conversion\n",
    "    return np.array(xs), np.array(ys), np.array(prices)\n",
    "\n",
    "X_train, y_train, train_prev_prices = create_sequences(train_data)\n",
    "X_test, y_test, test_prev_prices = create_sequences(test_data)\n",
    "\n",
    "print(f\"Train sequences: {X_train.shape}, Test sequences: {X_test.shape}\")\n",
    "\n",
    "# PyTorch Dataset\n",
    "class FinanceDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "train_ds = FinanceDataset(X_train, y_train)\n",
    "test_ds = FinanceDataset(X_test, y_test)\n",
    "train_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_ds, batch_size=64, shuffle=False)\n",
    "\n",
    "# LSTM Model\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size=2, hidden_size=64, num_layers=2):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_size, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x, _ = self.lstm(x)\n",
    "        x = x[:, -1]  # Last timestep\n",
    "        return self.linear(x)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = LSTMModel().to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Training\n",
    "EPOCHS = 50\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch).squeeze()\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f'Epoch {epoch+1}/{EPOCHS} Loss: {loss.item():.6f}')\n",
    "\n",
    "# Evaluation\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # Predict log returns\n",
    "    test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "    preds_scaled = model(test_tensor).cpu().numpy().squeeze()\n",
    "    \n",
    "    # Inverse transform to actual log returns\n",
    "    pred_log_ret = vix_scaler.inverse_transform(preds_scaled.reshape(-1, 1)).flatten()\n",
    "    true_log_ret = vix_scaler.inverse_transform(y_test.reshape(-1, 1)).flatten()\n",
    "    \n",
    "    # Convert log returns to prices\n",
    "    pred_prices = test_prev_prices * np.exp(pred_log_ret)\n",
    "    true_prices = test_data['Close_vix'].iloc[SEQ_LEN:].values  # Actual target prices\n",
    "    \n",
    "    # Calculate metrics\n",
    "    logret_r2 = r2_score(true_log_ret, pred_log_ret)\n",
    "    logret_rmse = np.sqrt(mean_squared_error(true_log_ret, pred_log_ret))\n",
    "    price_rmse = np.sqrt(mean_squared_error(true_prices, pred_prices))\n",
    "\n",
    "print(\"\\nLog Return Metrics:\")\n",
    "print(f\"RÂ²: {logret_r2:.4f}, RMSE: {logret_rmse:.6f}\")\n",
    "print(\"\\nPrice Metrics:\")\n",
    "print(f\"Price RMSE: {price_rmse:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
