{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4cc0444",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "c:\\Users\\danch\\anaconda3\\Lib\\site-packages\\arch\\univariate\\base.py:309: DataScaleWarning: y is poorly scaled, which may affect convergence of the optimizer when\n",
      "estimating the model parameters. The scale of y is 0.0001465. Parameter\n",
      "estimation work better when this value is between 1 and 1000. The recommended\n",
      "rescaling is 100 * y.\n",
      "\n",
      "This warning can be disabled by either rescaling y before initializing the\n",
      "model or by setting rescale=False.\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\danch\\AppData\\Local\\Temp\\ipykernel_22192\\38965785.py:54: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_data['vix_log_ret_scaled'] = vix_scaler.fit_transform(train_data[['vix_log_ret']])\n",
      "C:\\Users\\danch\\AppData\\Local\\Temp\\ipykernel_22192\\38965785.py:55: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_data['vix_log_ret_scaled'] = vix_scaler.transform(test_data[['vix_log_ret']])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: (5032, 4)\n",
      "            sp500_close  sp500_log_ret  vix_close  vix_log_ret\n",
      "Date                                                          \n",
      "2005-01-04  1188.050049      -0.011740      13.98    -0.007128\n",
      "2005-01-05  1183.739990      -0.003634      14.09     0.007838\n",
      "2005-01-06  1187.890015       0.003500      13.58    -0.036867\n",
      "2005-01-07  1186.189941      -0.001432      13.49    -0.006649\n",
      "2005-01-10  1190.250000       0.003417      13.23    -0.019462\n",
      "                     Constant Mean - GARCH Model Results                      \n",
      "==============================================================================\n",
      "Dep. Variable:          sp500_log_ret   R-squared:                       0.000\n",
      "Mean Model:             Constant Mean   Adj. R-squared:                  0.000\n",
      "Vol Model:                      GARCH   Log-Likelihood:                5067.30\n",
      "Distribution:                  Normal   AIC:                          -10126.6\n",
      "Method:            Maximum Likelihood   BIC:                          -10100.5\n",
      "                                        No. Observations:                 5032\n",
      "Date:                Mon, Jul 28 2025   Df Residuals:                     5031\n",
      "Time:                        00:46:06   Df Model:                            1\n",
      "                                 Mean Model                                 \n",
      "============================================================================\n",
      "                 coef    std err          t      P>|t|      95.0% Conf. Int.\n",
      "----------------------------------------------------------------------------\n",
      "mu             0.0852  2.520e-03     33.797 2.220e-250 [8.021e-02,9.009e-02]\n",
      "                               Volatility Model                              \n",
      "=============================================================================\n",
      "                 coef    std err          t      P>|t|       95.0% Conf. Int.\n",
      "-----------------------------------------------------------------------------\n",
      "omega      2.9315e-06  1.592e-05      0.184      0.854 [-2.826e-05,3.413e-05]\n",
      "alpha[1]       0.1000  7.037e-03     14.210  7.910e-46    [8.621e-02,  0.114]\n",
      "beta[1]        0.8800  9.464e-03     92.986      0.000      [  0.861,  0.899]\n",
      "=============================================================================\n",
      "\n",
      "Covariance estimator: robust\n",
      "X_train shape: (2989, 30, 2), y_train shape: (2989,)\n",
      "X_test shape: (1983, 30, 2), y_test shape: (1983,)\n",
      "Epoch 10/50 | Loss: 0.984787\n",
      "Epoch 20/50 | Loss: 0.980640\n",
      "Epoch 30/50 | Loss: 0.963637\n",
      "Epoch 40/50 | Loss: 0.922905\n",
      "Epoch 50/50 | Loss: 0.865940\n",
      "\n",
      "Log Return Metrics:\n",
      "RÂ²: -0.0853\n",
      "RMSE: 0.083520\n",
      "\n",
      "Price Metrics:\n",
      "Price RMSE: 2.1828\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from arch import arch_model\n",
    "\n",
    "# Download data\n",
    "start_date = '2005-01-01'\n",
    "end_date = '2025-01-01'\n",
    "\n",
    "# Get S&P 500 data\n",
    "sp500 = yf.download('^GSPC', start=start_date, end=end_date, auto_adjust=True)\n",
    "sp500 = sp500[['Close']].copy()\n",
    "sp500.columns = ['sp500_close']\n",
    "sp500['sp500_log_ret'] = np.log(sp500['sp500_close'] / sp500['sp500_close'].shift(1))\n",
    "\n",
    "# Get VIX data\n",
    "vix = yf.download('^VIX', start=start_date, end=end_date, auto_adjust=True)\n",
    "vix = vix[['Close']].copy()\n",
    "vix.columns = ['vix_close']\n",
    "vix['vix_log_ret'] = np.log(vix['vix_close'] / vix['vix_close'].shift(1))\n",
    "\n",
    "# Merge datasets on date index\n",
    "data = pd.merge(sp500, vix, left_index=True, right_index=True, how='inner')\n",
    "data.dropna(inplace=True)  # Remove first row with NaN returns\n",
    "print(f\"Data shape: {data.shape}\")\n",
    "print(data.head())\n",
    "\n",
    "# Fit GARCH model on S&P 500 log returns\n",
    "garch_model = arch_model(\n",
    "    data['sp500_log_ret'], \n",
    "    mean='Constant',  # Important for proper residual calculation\n",
    "    vol='GARCH', \n",
    "    p=1, \n",
    "    q=1\n",
    ")\n",
    "garch_result = garch_model.fit(update_freq=0, disp='off')\n",
    "print(garch_result.summary())\n",
    "\n",
    "# Add standardized residuals to dataframe\n",
    "data['garch_resid'] = garch_result.std_resid\n",
    "\n",
    "# Train-test split (60-40)\n",
    "split_idx = int(0.6 * len(data))\n",
    "train_data = data.iloc[:split_idx]\n",
    "test_data = data.iloc[split_idx:]\n",
    "\n",
    "# Scale VIX log returns using training data\n",
    "vix_scaler = StandardScaler()\n",
    "train_data['vix_log_ret_scaled'] = vix_scaler.fit_transform(train_data[['vix_log_ret']])\n",
    "test_data['vix_log_ret_scaled'] = vix_scaler.transform(test_data[['vix_log_ret']])\n",
    "\n",
    "# Sequence creation\n",
    "SEQ_LEN = 30\n",
    "\n",
    "def create_sequences(df):\n",
    "    xs, ys = [], []\n",
    "    for i in range(SEQ_LEN, len(df)):\n",
    "        # Input: [GARCH residual, Scaled VIX return]\n",
    "        x = df[['garch_resid', 'vix_log_ret_scaled']].iloc[i-SEQ_LEN:i].values\n",
    "        # Target: Next period's scaled VIX return\n",
    "        y = df['vix_log_ret_scaled'].iloc[i]\n",
    "        xs.append(x)\n",
    "        ys.append(y)\n",
    "    return np.array(xs), np.array(ys)\n",
    "\n",
    "X_train, y_train = create_sequences(train_data)\n",
    "X_test, y_test = create_sequences(test_data)\n",
    "\n",
    "print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")\n",
    "\n",
    "# PyTorch Dataset\n",
    "class FinanceDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "train_ds = FinanceDataset(X_train, y_train)\n",
    "test_ds = FinanceDataset(X_test, y_test)\n",
    "train_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_ds, batch_size=64, shuffle=False)\n",
    "\n",
    "# LSTM Model (same as before)\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size=2, hidden_size=64, num_layers=2):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_size, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x, _ = self.lstm(x)\n",
    "        x = x[:, -1]  # Last timestep\n",
    "        return self.linear(x)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = LSTMModel().to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Training loop\n",
    "EPOCHS = 50\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch).squeeze()\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    if (epoch+1) % 10 == 0:\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        print(f'Epoch {epoch+1}/{EPOCHS} | Loss: {avg_loss:.6f}')\n",
    "\n",
    "# Evaluation and price conversion\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # Predict scaled log returns\n",
    "    test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "    preds_scaled = model(test_tensor).cpu().numpy().squeeze()\n",
    "    \n",
    "    # Inverse scaling to actual log returns\n",
    "    pred_log_ret = vix_scaler.inverse_transform(preds_scaled.reshape(-1, 1)).flatten()\n",
    "    true_log_ret = test_data['vix_log_ret'].iloc[SEQ_LEN:].values\n",
    "    \n",
    "    # Convert log returns to prices\n",
    "    # Get previous closing prices for conversion\n",
    "    prev_prices = test_data['vix_close'].iloc[SEQ_LEN-1:-1].values\n",
    "    pred_prices = prev_prices * np.exp(pred_log_ret)\n",
    "    true_prices = test_data['vix_close'].iloc[SEQ_LEN:].values\n",
    "    \n",
    "    # Calculate metrics\n",
    "    logret_r2 = r2_score(true_log_ret, pred_log_ret)\n",
    "    logret_rmse = np.sqrt(mean_squared_error(true_log_ret, pred_log_ret))\n",
    "    price_rmse = np.sqrt(mean_squared_error(true_prices, pred_prices))\n",
    "\n",
    "print(\"\\nLog Return Metrics:\")\n",
    "print(f\"RÂ²: {logret_r2:.4f}\")\n",
    "print(f\"RMSE: {logret_rmse:.6f}\")\n",
    "\n",
    "print(\"\\nPrice Metrics:\")\n",
    "print(f\"Price RMSE: {price_rmse:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
