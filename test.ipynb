{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bdfa6b13",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "c:\\Users\\danch\\anaconda3\\Lib\\site-packages\\arch\\univariate\\base.py:309: DataScaleWarning: y is poorly scaled, which may affect convergence of the optimizer when\n",
      "estimating the model parameters. The scale of y is 0.0001465. Parameter\n",
      "estimation work better when this value is between 1 and 1000. The recommended\n",
      "rescaling is 100 * y.\n",
      "\n",
      "This warning can be disabled by either rescaling y before initializing the\n",
      "model or by setting rescale=False.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\danch\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:610: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VIX Number of NA's: 0\n",
      "            Adj_Close  Close   High    Low   Open  log_returns\n",
      "Date                                                          \n",
      "2005-01-04      13.98  13.98  14.45  13.93  14.01    -0.007128\n",
      "2005-01-05      14.09  14.09  14.09  13.26  13.98     0.007838\n",
      "2005-01-06      13.58  13.58  14.09  13.33  14.09    -0.036867\n",
      "2005-01-07      13.49  13.49  13.51  12.94  13.47    -0.006649\n",
      "2005-01-10      13.23  13.23  13.93  12.94  13.92    -0.019462\n",
      "SP500 Number of NA's: 0\n",
      "              Adj_Close        Close         High          Low         Open  \\\n",
      "Date                                                                          \n",
      "2005-01-04  1188.050049  1188.050049  1205.839966  1185.390015  1202.079956   \n",
      "2005-01-05  1183.739990  1183.739990  1192.729980  1183.719971  1188.050049   \n",
      "2005-01-06  1187.890015  1187.890015  1191.630005  1183.270020  1183.739990   \n",
      "2005-01-07  1186.189941  1186.189941  1192.199951  1182.160034  1187.890015   \n",
      "2005-01-10  1190.250000  1190.250000  1194.780029  1184.800049  1186.189941   \n",
      "\n",
      "            log_returns  Volatility  \n",
      "Date                                 \n",
      "2005-01-04    -0.011740    0.017252  \n",
      "2005-01-05    -0.003634    0.007612  \n",
      "2005-01-06     0.003500    0.007065  \n",
      "2005-01-07    -0.001432    0.008493  \n",
      "2005-01-10     0.003417    0.008423  \n",
      "Iteration:      1,   Func. Count:      5,   Neg. LLF: 2812479369.4380984\n",
      "Iteration:      2,   Func. Count:     10,   Neg. LLF: -16452.322351287017\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: -16452.322346143374\n",
      "            Iterations: 6\n",
      "            Function evaluations: 10\n",
      "            Gradient evaluations: 2\n",
      "Fitting done!\n",
      "\n",
      "                       Zero Mean - GARCH Model Results                        \n",
      "==============================================================================\n",
      "Dep. Variable:            log_returns   R-squared:                       0.000\n",
      "Mean Model:                 Zero Mean   Adj. R-squared:                  0.000\n",
      "Vol Model:                      GARCH   Log-Likelihood:                16452.3\n",
      "Distribution:                  Normal   AIC:                          -32898.6\n",
      "Method:            Maximum Likelihood   BIC:                          -32879.1\n",
      "                                        No. Observations:                 5032\n",
      "Date:                Mon, Jul 28 2025   Df Residuals:                     5032\n",
      "Time:                        00:34:05   Df Model:                            0\n",
      "                              Volatility Model                              \n",
      "============================================================================\n",
      "                 coef    std err          t      P>|t|      95.0% Conf. Int.\n",
      "----------------------------------------------------------------------------\n",
      "omega      2.8902e-06  2.492e-09   1159.569      0.000 [2.885e-06,2.895e-06]\n",
      "alpha[1]       0.0986  2.548e-02      3.869  1.092e-04   [4.865e-02,  0.149]\n",
      "beta[1]        0.8676  1.999e-02     43.399      0.000     [  0.828,  0.907]\n",
      "============================================================================\n",
      "\n",
      "Covariance estimator: robust\n",
      "(5032, 1)\n",
      "(5032, 1)\n",
      "X shape: (5002, 30, 2)\n",
      "y shape: (5002,)\n",
      "(3001, 30, 2) (2001, 30, 2)\n",
      "(3001,) (2001,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\danch\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:610: UserWarning: Using a target size (torch.Size([57])) that is different to the input size (torch.Size([57, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/50, Training Loss: 0.919423\n",
      "Epoch 10/50, Training Loss: 0.919346\n",
      "Epoch 15/50, Training Loss: 0.919194\n",
      "Epoch 20/50, Training Loss: 0.919120\n",
      "Epoch 25/50, Training Loss: 0.919040\n",
      "Epoch 30/50, Training Loss: 0.919060\n",
      "Epoch 35/50, Training Loss: 0.918922\n",
      "Epoch 40/50, Training Loss: 0.918767\n",
      "Epoch 45/50, Training Loss: 0.918708\n",
      "Epoch 50/50, Training Loss: 0.918671\n",
      "R^2: -0.0086\n",
      "RMSE: 0.0803\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import yfinance\n",
    "\n",
    "import torch\n",
    "from torch import optim\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "from arch import arch_model\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "start_date = '2005-01-01'\n",
    "end_date = '2025-01-01'\n",
    "\n",
    "vix = yfinance.download('^VIX', start=start_date, end=end_date, interval='1d', auto_adjust=False)\n",
    "vix.columns = ['Adj_Close', 'Close', 'High', 'Low', 'Open', 'Volume']\n",
    "vix.drop(columns='Volume', inplace=True)\n",
    "vix['log_returns'] = np.log(vix['Close']/vix['Close'].shift(1))\n",
    "vix.dropna(inplace=True)\n",
    "print(f\"VIX Number of NA's: {vix.isna().sum().sum()}\")\n",
    "print(vix.head())\n",
    "\n",
    "sp500 = yfinance.download('^GSPC', start=start_date, end=end_date, interval='1d', auto_adjust=False)\n",
    "sp500.columns = ['Adj_Close', 'Close', 'High', 'Low', 'Open', 'Volume']\n",
    "sp500.drop(columns='Volume', inplace=True)\n",
    "sp500['log_returns'] = np.log(sp500['Close']/sp500['Close'].shift(1))\n",
    "sp500.dropna(inplace=True)\n",
    "sp500['Volatility'] = sp500['High']/sp500['Low'] - 1\n",
    "\n",
    "print(f\"SP500 Number of NA's: {sp500.isna().sum().sum()}\")\n",
    "print(sp500.head())\n",
    "\n",
    "garch_model = arch_model(y=sp500['log_returns'], x=sp500, mean='Zero', vol='GARCH', p=1, q=1)\n",
    "\n",
    "garch_result = garch_model.fit()\n",
    "\n",
    "print(\"Fitting done!\\n\")\n",
    "\n",
    "print(garch_result.summary())\n",
    "forecast = garch_result.forecast(params=garch_result.params, horizon=10)\n",
    "forecast.variance\n",
    "garch_std_resid = garch_result.std_resid.values.reshape(-1, 1)\n",
    "vix_log_returns = vix['log_returns'].values.reshape(-1, 1)\n",
    "\n",
    "print(garch_std_resid.shape)\n",
    "print(vix_log_returns.shape)\n",
    "\n",
    "\n",
    "# Some reason the length is off by 1 T^T (5032 vs 5033), idk why\n",
    "min_len = min(len(garch_std_resid), len(vix_log_returns))\n",
    "garch_std_resid = garch_std_resid[-min_len:]\n",
    "vix_log_returns = vix_log_returns[-min_len:]\n",
    "\n",
    "# GARCH standardized residuals are already standardized, so no need for StandardScaler\n",
    "# Only scale the VIX log returns\n",
    "vix_scaler = StandardScaler().fit(vix_log_returns)\n",
    "vix_log_returns_norm = vix_scaler.transform(vix_log_returns)\n",
    "\n",
    "features = np.concatenate([garch_std_resid, vix_log_returns_norm], axis=1)\n",
    "\n",
    "\n",
    "def create_sequences(data, seq_length):\n",
    "    xs = []\n",
    "    ys = []\n",
    "    for i in range(len(data) - seq_length):\n",
    "        x = data[i:(i+seq_length)]\n",
    "        y = data[i+seq_length, 1]\n",
    "        xs.append(x)\n",
    "        ys.append(y)\n",
    "    return np.array(xs), np.array(ys)\n",
    "\n",
    "SEQ_LEN = 30\n",
    "X, y = create_sequences(features, SEQ_LEN)\n",
    "\n",
    "print(\"X shape:\", X.shape)\n",
    "print(\"y shape:\", y.shape)\n",
    "\n",
    "# Train/test split\n",
    "split = int(0.6 * len(X))\n",
    "X_train, X_test = X[:split], X[split:]\n",
    "y_train, y_test = y[:split], y[split:]\n",
    "\n",
    "print(X_train.shape, X_test.shape)\n",
    "print(y_train.shape, y_test.shape)\n",
    "\n",
    "\n",
    "\n",
    "class StockDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32)\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "train_ds = StockDataset(X_train, y_train)\n",
    "test_ds = StockDataset(X_test, y_test)\n",
    "train_loader = DataLoader(train_ds, batch_size=64, shuffle=False)\n",
    "test_loader = DataLoader(test_ds, batch_size=64, shuffle=False)\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size=2, hidden_size=64, num_layers=2):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc1 = nn.Linear(hidden_size, 1)\n",
    "        # self.fc2 = nn.Linear(hidden_size // 2, 1)\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        out = out[:, -1, :]  # Take last timestep\n",
    "        out = self.fc1(out)\n",
    "        # out = self.fc2(out)\n",
    "        return out\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = LSTMModel().to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "EPOCHS = 50\n",
    "train_losses = []\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for xb, yb in train_loader:\n",
    "        xb = xb.to(device)\n",
    "        yb = yb.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        out = model(xb)\n",
    "        loss = criterion(out, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss = epoch_loss + loss.item()*xb.size(0)\n",
    "    \n",
    "    avg_loss = epoch_loss / len(train_loader.dataset)\n",
    "    train_losses.append(avg_loss)\n",
    "    if epoch%5 == 0: print(f\"Epoch {epoch}/{EPOCHS}, Training Loss: {avg_loss:.6f}\")\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "    preds = model(X_test_tensor).cpu().numpy()\n",
    "\n",
    "    preds_vix = preds.reshape(-1, 1)\n",
    "    y_test_vix = y_test.reshape(-1, 1)\n",
    "    preds_inv = vix_scaler.inverse_transform(preds_vix)\n",
    "    y_test_inv = vix_scaler.inverse_transform(y_test_vix)\n",
    "\n",
    "r2 = r2_score(y_test_inv, preds_inv)\n",
    "rmse = np.sqrt(mean_squared_error(y_test_inv, preds_inv))\n",
    "print(f\"R^2: {r2:.4f}\")\n",
    "print(f\"RMSE: {rmse:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
